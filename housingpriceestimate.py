# -*- coding: utf-8 -*-
"""HousingPriceEstimate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pH-V4wcFb1YF5anqIxT6IDKo3PoN4CvE

#Housing Price Estimate

## Imports
"""

import pandas as pd
import numpy as np

from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import mean_squared_log_error
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
import lightgbm as lgb

import matplotlib.pyplot as plt
import seaborn as sns

"""### Dataset is from kaggle: https://www.kaggle.com/competitions/home-data-for-ml-course/overview"""

train = pd.read_csv('train.csv')

print(train.head())

test = pd.read_csv('test.csv')

print(test.head())

submission = pd.read_csv('sample_submission.csv')

print(submission.head())

"""## Preprocessing!

### Let's combine Train/Test for Uniform Preprocessing
"""

train['is_train'] = 1
test['is_train'] = 0
test['SalePrice'] = np.nan  # add dummy target

full = pd.concat([train, test], sort=False)

# Fill categorical NAs with "None"
categorical_fill_none = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',
                         'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish',
                         'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature', 'MasVnrType']

for col in categorical_fill_none:
    full[col] = full[col].fillna("None")

# Fill numerical NAs with 0 or median
numerical_fill_zero = ['GarageYrBlt', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',
                       'TotalBsmtSF', 'GarageCars', 'GarageArea']

for col in numerical_fill_zero:
    full[col] = full[col].fillna(0)

# Fill LotFrontage by median in each neighborhood
full['LotFrontage'] = full.groupby("Neighborhood")["LotFrontage"].transform(lambda x: x.fillna(x.median()))

# Drop columns with too many NAs
full.drop(['Utilities'], axis=1, inplace=True)

"""### Feature Engineering"""

# Total square footage
full['TotalSF'] = full['TotalBsmtSF'] + full['1stFlrSF'] + full['2ndFlrSF']
full['TotalBathrooms'] = (full['FullBath'] + (0.5 * full['HalfBath']) +
                          full['BsmtFullBath'] + (0.5 * full['BsmtHalfBath']))
full['Age'] = full['YrSold'] - full['YearBuilt']
full['Remodeled'] = (full['YearRemodAdd'] != full['YearBuilt']).astype(int)

"""### Encoding"""

# Label encode ordinal features
label_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual',
              'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']
for col in label_cols:
    le = LabelEncoder()
    full[col] = le.fit_transform(full[col])

# One-hot encode nominal features
full = pd.get_dummies(full)

"""### Split Train/Test Back"""

train = full[full['is_train'] == 1].drop(['is_train'], axis=1)
test = full[full['is_train'] == 0].drop(['is_train', 'SalePrice'], axis=1)

X = train.drop(['SalePrice', 'Id'], axis=1)
y = train['SalePrice']

"""### ðŸ˜¨ NAN Values!!! This is an error fix I found training the models the first time"""

# Find columns with missing values
missing_cols = test.columns[test.isnull().any()]
print(test[missing_cols].isnull().sum())

# Fill any remaining NaNs with median (usually a safe default)
test.fillna(test.median(numeric_only=True), inplace=True)

missing_cols = test.columns[test.isnull().any()]
print(test[missing_cols].isnull().sum())

"""## Model Time"""

# Define cross-validation
def rmsle_cv(model):
    kf = KFold(5, shuffle=True, random_state=42)
    rmse = np.sqrt(-cross_val_score(model, X, y, scoring="neg_mean_squared_error", cv=kf))
    return rmse

# Ridge Regression
ridge = Ridge(alpha=20)
print("Ridge RMSLE:", rmsle_cv(ridge).mean())

# XGBoost
xgb_model = xgb.XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=3)
print("XGBoost RMSLE:", rmsle_cv(xgb_model).mean())

xgb_model.fit(X, y)
ridge.fit(X, y)

# Predict
xgb_preds = xgb_model.predict(test.drop('Id', axis=1))
ridge_preds = ridge.predict(test.drop('Id', axis=1))

# Blend predictions
final_preds = (xgb_preds * 0.7) + (ridge_preds * 0.3)

from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import mean_squared_error
import numpy as np

def rmse_cv(model):
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    rmse = np.sqrt(-cross_val_score(model, X, y, scoring="neg_mean_squared_error", cv=kf))
    return rmse

print("Ridge CV RMSE: {:.4f}".format(rmse_cv(ridge).mean()))
print("XGBoost CV RMSE: {:.4f}".format(rmse_cv(xgb_model).mean()))

"""Both models off by around $30000 per prediction...not too bad without tuning"""

xgb.plot_importance(xgb_model, max_num_features=15)
plt.title("Top 15 Feature Importances - XGBoost")
plt.show()

y_log = np.log1p(y)

def rmse_cv(model):
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    return np.sqrt(-cross_val_score(model, X, y_log, scoring="neg_mean_squared_error", cv=kf))

# Train models with y_log
xgb_model.fit(X, y_log)
ridge.fit(X, y_log)

# Predict and inverse transform
xgb_preds = np.expm1(xgb_model.predict(test.drop('Id', axis=1)))
ridge_preds = np.expm1(ridge.predict(test.drop('Id', axis=1)))
final_preds = 0.7 * xgb_preds + 0.3 * ridge_preds

print("Ridge log-RMSE: {:.4f}".format(rmse_cv(ridge).mean()))
print("XGBoost log-RMSE: {:.4f}".format(rmse_cv(xgb_model).mean()))

"""## Improving Accuracy???

### Trying LightGBM
"""

"""## Improving Accuracy Further"""

# LightGBM Model
lgb_model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05, max_depth=3, random_state=42)
print("LightGBM log-RMSE: {:.4f}".format(rmse_cv(lgb_model).mean()))

# Train and predict
lgb_model.fit(X, y_log)
lgb_preds = np.expm1(lgb_model.predict(test.drop('Id', axis=1)))

# Compare model performance
print("\nModel Comparison (log-RMSE):")
print(f" - Ridge     : {rmse_cv(ridge).mean():.4f}")
print(f" - XGBoost   : {rmse_cv(xgb_model).mean():.4f}")
print(f" - LightGBM  : {rmse_cv(lgb_model).mean():.4f}")

# Blend predictions
final_preds_blended = (xgb_preds * 0.5) + (ridge_preds * 0.2) + (lgb_preds * 0.3)

# Compare model performance
print("\nModel Comparison (log-RMSE):")
print(f" - Ridge     : {rmse_cv(ridge).mean():.4f}")
print(f" - XGBoost   : {rmse_cv(xgb_model).mean():.4f}")
print(f" - LightGBM  : {rmse_cv(lgb_model).mean():.4f}")

"""### XGBoost Tuning"""

y_log = np.log1p(y)

def rmsle_cv(model):
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    return np.sqrt(-cross_val_score(model, X, y_log, scoring="neg_mean_squared_error", cv=kf))

xgb_model = xgb.XGBRegressor(
    n_estimators=3000,
    learning_rate=0.01,
    max_depth=4,
    subsample=0.7,
    colsample_bytree=0.7,
    gamma=0,
    reg_alpha=0.1,
    reg_lambda=1,
    random_state=42
)

print("XGBoost (Tuned) log-RMSE: {:.4f}".format(rmsle_cv(xgb_model).mean()))

# Fit and predict
xgb_model.fit(X, y_log)
xgb_preds = np.expm1(xgb_model.predict(test.drop('Id', axis=1)))